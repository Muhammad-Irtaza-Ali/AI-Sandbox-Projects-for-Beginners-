# ğŸ§  Neural Network Forward Propagation  

This project demonstrates **forward propagation** in a simple **3-node hidden layer neural network**, with and without **activation functions**.  

## ğŸ“Œ Overview  
- The network processes an **input array** `[1,1]`.  
- Weights are manually defined for each node.  
- The **output is computed** using different activation functions:  
  1ï¸âƒ£ **Without Activation Function**  
  2ï¸âƒ£ **With Tanh Activation**  
  3ï¸âƒ£ **With ReLU Activation**  

## âš™ï¸ Network Structure  
âœ”ï¸ **Input Layer**: `[1,1]`  
âœ”ï¸ **Hidden Layer**: 3 nodes  
âœ”ï¸ **Output Layer**: 1 node  

## ğŸ”¢ Computation  
### ğŸŸ¢ **Without Activation Function**  
Each node computes a weighted sum:  
\[
\text{Node Output} = \sum (\text{Input} \times \text{Weight})
\]  
Final output is obtained by multiplying the **hidden layer output** with the **output weights**.  

### ğŸ”µ **With Tanh Activation**  
Applies **tanh** function to introduce non-linearity:  
\[
\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]  

### ğŸ”´ **With ReLU Activation**  
Applies **ReLU (Rectified Linear Unit)** function:  
\[
\text{ReLU}(x) = \max(0, x)
\]  

## ğŸš€ How to Run  
1ï¸âƒ£ Install dependencies:  
   ```bash
   pip install numpy
   ```
2ï¸âƒ£ Run the Python script.  
3ï¸âƒ£ Observe **different outputs** based on the activation function used.  

## ğŸ“Š Key Takeaways  
âœ”ï¸ **Activation functions help neural networks learn complex patterns.**  
âœ”ï¸ **Tanh is useful for values centered around zero.**  
âœ”ï¸ **ReLU is efficient and prevents vanishing gradients.**  

## ğŸ‘¤ Author  
This project is maintained by **Muhammad Irtaza Ali** as part of his AI learning journey.  

ğŸ“Œ **Happy Coding! ğŸš€**  

